---
title: تکنیک LoRA
draft: false
tags: 
date: 2024-09-09 11:02
---

عبارت **LoRA** مخفف Low Rank Adaptation (همگام‌سازی با رتبه پایین) است. تکنیک LoRA در سال ۲۰۲۱ و در [این مقاله](https://arxiv.org/abs/2106.09685) به عنوان یک راه بهینه برای [[Fine-tuning]] مدل‌ها معرفی شده است. پیش‌فرض اصلی تکنیک این است که [[رتبهٔ ماتریس]] [[Model Weights|وزن‌های مدل]] به قدری پایین است که می‌توان با یک ماتریس به مراتب کوچک‌تر، عملکرد آن را تکرار کرد.

## کارکرد LoRA

تکنیک LoRA بر خلاف [[Fine-tuning]] کلاسیک، نیازی به تنظیم کل مدل ندارد. در مدل کلاسیک، کل [[Model Weights|وزن‌های مدل]] یک دور از ابتدا محاسبه می‌شوند که فرآیندی بسیار زمان‌بر و هزینه‌زاست.

در تکنیک LoRA، معماری [[شبکهٔ عصبی]] به دو ماتریس هم‌رده تقسیم می‌شود. یک ماتریس همان مدل اصلی است که به اصطلاح «فریز[^1]» می‌شود و دست‌نخورده به شکل Read-Only باقی می‌ماند؛ موازی با این ماتریس، دو ماتریس دیگر با نام‌های `A` و `B` تشکیل می‌دهیم. اگر ماتریس اصلی `W` به مرتبهٔ `d x k` باشد، آن‌گاه ماتریس `B` به مرتبهٔ `d x r` و ماتریس A به مرتبهٔ `r x k` خواهد بود که ضرب `BA` در نهایت ماتریس `ΔW`  با مرتبهٔ `d x k` خروجی دهد. مقدار انتخاب شده برای `r` همیشه برابر عددی بسیار کوچک‌تر از `d` و `k` خواهد بود. این مقدار کم باعث می‌شود که ماتریس BA با داشتن پارامترهای کمتر، همچنان خروجی به مرتبهٔ `d x k` تولید می‌کند[^2].

<iframe width="100%" height="300px" src="https://eledah.github.io/quartz_blog/attachment/lora.html"></iframe>

- از ترکیب این دو ماتریس، یک خروجی واحد تولید می‌شود. مقدار [[Loss]] این خروجی تعیین شده و با تکنیک [[Backpropagation|انتشار معکوس]]، [[Model Weights|وزن‌های ماتریس BA]] اصلاح می‌شود. دقت کنید که در این حالت مدل اصلی در این حالت هیچ تغییری نمی‌کند.


> [!example] منابع بیشتر
> - ویدئوی [LoRA: Low-Rank Adaptation of Large Language Models](https://www.youtube.com/watch?v=PXWYUTMt-AU)
> - ویدئوی [Explaining the Key Concepts Behind LoRA](https://www.youtube.com/watch?v=dA-NhCtrrVE)


[^1]: Freeze
[^2]: فرضاً با `d = 1000`، `k = 3000` و `r = 5`، پارامترهای مدل اصلی برابر ۳٫۰۰۰٫۰۰۰ است. اما پارامترهای ماتریس‌های B و A مجموعاً  ۲۳٫۰۰۰ هستند. یعنی کمتر از ۱٪. پارامتر کمتر یعنی حجم کمتر، [[Backpropagation|انتشار معکوس]] سریعتر و سرعت بیشتر در اجرا.